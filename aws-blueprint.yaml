tosca_definitions_version: cloudify_dsl_1_3

description: >
  This blueprint creates a Kubernetes Cluster.
  It is based on this documentation: https://kubernetes.io/docs/getting-started-guides/kubeadm/

imports:
  - http://www.getcloudify.org/spec/cloudify/4.0.1/types.yaml
  - http://getcloudify.org.s3.amazonaws.com/spec/aws-plugin/1.4.9/plugin.yaml
  - http://www.getcloudify.org/spec/diamond-plugin/1.3.5/plugin.yaml
  - types/scale.yaml
  - types/cloud_config/cloud-config.yaml
  - types/kubernetes.yaml
  - imports/cloud-config.yaml
  - ubscfylib/common/types.yaml
  - imports/kubernetes.yaml

  # Move this to be located on the manager as part of the install?
  - imports/cloudify_manager.yaml

inputs:

  ami:
    description: >
      An AWS AMI. Tested with a Centos 7.0 image.
    default: ami-f4533694 # centos7 us-west-2

  instance_type:
    description: >
      The AWS instance_type. Tested with m3.medium, although that is unnecessarily large.
    default: m3.medium

  agent_user:
    description: >
      The username of the agent running on the instance created from the image.
    default: centos

  encode_cloud_config:
    default: false

dsl_definitions:

    aws_config: &aws_config
      aws_access_key_id: { get_secret: aws_access_key_id }
      aws_secret_access_key: { get_secret: aws_secret_access_key }
      ec2_region_name: { get_secret: ec2_region_name }
      ec2_region_endpoint: { get_secret: ec2_region_endpoint }
    agent_config: &agent_config
      user: { get_input: agent_user }
      key: '/home/centos/agent-priv.pem'
      install_method: remote
      extra:
        # Alternate agent deployment
        package_url: 
          concat:
            - get_property: [_cloudify_manager, file_server_url]
            - packages/agents/centos-core-agent-mterrel0626.tar.gz
    default_root_block_device_map: &default_root_block_device_map
      /dev/sda1:
        size: 8
        delete_on_termination: true
    aws_k8s_host_common_relationships:
      - &sg_ssh
        type: cloudify.aws.relationships.instance_connected_to_security_group
        target: ssh_group
      - &sg_agent
        type: cloudify.aws.relationships.instance_connected_to_security_group
        target: agent_security_group
      - &sg_kube
        type: cloudify.aws.relationships.instance_connected_to_security_group
        target: kubernetes_security_group
      - &agent_modules
        type: ubs.relationships.agent_modules_from
        target: ubs.nodes.default_modules



node_types:

  aws_k8s_host_common:
    derived_from: cloudify.aws.nodes.Instance
    properties:
      agent_config:
        default: *agent_config
      aws_config:
        default: *aws_config
      image_id:
        default: { get_input: ami }
      instance_type:
        default: { get_input: instance_type }
      parameters:
        default:
          block_device_map: *default_root_block_device_map
    interfaces:
      cloudify.interfaces.lifecycle:
        create:
          implementation: aws.cloudify_aws.ec2.instance.create
          inputs:
            args:
              default:
                placement: { get_secret: availability_zone }
                user_data: { get_attribute: [ cloudify_host_cloud_config, cloud_config ] }
      cloudify.interfaces.monitoring_agent:
        install:
          implementation: diamond.diamond_agent.tasks.install
          inputs:
            diamond_config:
              default:
                interval: 1
        start: diamond.diamond_agent.tasks.start
        stop: diamond.diamond_agent.tasks.stop
        uninstall: diamond.diamond_agent.tasks.uninstall
      cloudify.interfaces.monitoring:
        start:
          implementation: diamond.diamond_agent.tasks.add_collectors
          inputs:
            collectors_config:
              default:
                ProcessResourcesCollector:
                  config:
                    enabled: true
                    unit: B
                    measure_collector_time: true
                    cpu_interval: 0.5
                    process:
                      hyperkube:
                        name: hyperkube

node_templates:

  #
  # Hosts
  #
  kubernetes_master_host:
    type: aws_k8s_host_common
    properties:
      name: k8s_master
    relationships:
      - *sg_ssh
      - *sg_agent
      - *sg_kube
      - *agent_modules
      - type: cloudify.aws.relationships.instance_connected_to_subnet
        target: public_subnet
      - type: cloudify.aws.relationships.instance_connected_to_elastic_ip
        target: kubernetes_master_ip

  kubernetes_node_host:
    type: aws_k8s_host_common
    properties:
      name: k8s_node
    relationships:
      - *sg_ssh
      - *sg_agent
      - *sg_kube
      - *agent_modules
      - type: cloudify.aws.relationships.instance_connected_to_subnet
        target: private_subnet

  #
  # Security Groups
  #
  agent_security_group:
    type: cloudify.aws.nodes.SecurityGroup
    properties:
      description: Cloudify agent security group
      use_external_resource: true
      resource_id: sg-7a193001
      aws_config: *aws_config

  kubernetes_security_group:
    type: cloudify.aws.nodes.SecurityGroup
    properties:
      aws_config: *aws_config
      description: Security group for Kubernetes Cluster
      rules:
        - ip_protocol: tcp
          from_port: 53
          to_port: 53
          cidr_ip: 0.0.0.0/0
        - ip_protocol: udp
          from_port: 53
          to_port: 53
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 80
          to_port: 80
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 443
          to_port: 443
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 2379
          to_port: 2379
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 4001
          to_port: 4001
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 6443
          to_port: 6443
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 8000
          to_port: 8000
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 8080
          to_port: 8080
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 9090
          to_port: 9090
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 10250
          to_port: 10250
          cidr_ip: 0.0.0.0/0
    relationships:
      - type: cloudify.aws.relationships.security_group_contained_in_vpc
        target: vpc

  ssh_group:
    type: cloudify.aws.nodes.SecurityGroup
    properties:
      aws_config: *aws_config
      description: SSH Group
      rules:
        - ip_protocol: tcp
          from_port: 22
          to_port: 22
          cidr_ip: 0.0.0.0/0
    relationships:
      - type: cloudify.aws.relationships.security_group_contained_in_vpc
        target: vpc

  #
  # Networking
  #
  kubernetes_master_ip:
    type: cloudify.aws.nodes.ElasticIP
    properties:
      aws_config: *aws_config
      domain: vpc

  public_subnet:
    type: cloudify.aws.nodes.Subnet
    properties:
      aws_config: *aws_config
      use_external_resource: true
      resource_id: { get_secret: public_subnet_id }
      cidr_block: N/A
      availability_zone: N/A
    relationships:
      - type: cloudify.aws.relationships.subnet_contained_in_vpc
        target: vpc

  private_subnet:
    type: cloudify.aws.nodes.Subnet
    properties:
      aws_config: *aws_config
      use_external_resource: true
      resource_id: { get_secret: private_subnet_id }
      cidr_block:  N/A
      availability_zone:  N/A
    relationships:
      - type: cloudify.aws.relationships.subnet_contained_in_vpc
        target: vpc

  vpc:
    type: cloudify.aws.nodes.VPC
    properties:
      aws_config: *aws_config
      use_external_resource: true
      resource_id: { get_secret: vpc_id }
      cidr_block: N/A
    relationships:
      - type: cloudify.relationships.depends_on
        target: cloudify_host_cloud_config


groups:

  k8s_node_scale_group:
    members:
      - kubernetes_node_host

policies:

  kubernetes_node_vms_scaling_policy:
    type: cloudify.policies.scaling
    properties:
      default_instances:  1
    targets: [k8s_node_scale_group]

outputs:
  kubectl_config:
    description: Full text of the configuration used to connect to the master.
    value: { get_attribute: [ kubernetes_master, configuration_file_content ] }

